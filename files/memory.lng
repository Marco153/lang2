CHUNK_FREE : const s32 = 1;
CHUNK_ALLOCATED : const s32= 4;

heap_hash : struct
{
	inner :struct
	{
		key : *u8,
		value : *u8,
	} 
	data : *inner,
}

mem_chunk : struct
{
    next : *mem_chunk,
    prev : *mem_chunk,
    size : u32,
    flags: u32,
    addr : *u8,
}
mem_alloc : struct
{
    buffer : *u8,
    head_free : *mem_chunk,
    in_use : heap_hash,
    all : *mem_chunk,
    probable_unallocated : **mem_chunk,
}

CHUNKS_CAP :const s32=  (512);
BYTES_PER_CHUNK : const s32= 8;
HASH_TABLE_SIZE : const s32=  (512);
UNALLOCATED_BUFFER_ITEMS : const s32=  8;

GetMem::fn outsider(sz:u64) ! *void;
Print::fn outsider(sz:*mem_alloc) ! *void;

heap_insert_unallocated::fn(alloc : *mem_alloc, a : *mem_chunk)
{
	if a < alloc.all || a > (ptr_offset(alloc.all, CHUNKS_CAP, mem_chunk)) || is_flag_on(a.flags, CHUNK_ALLOCATED)
		return;

	i:u64 = 0;

	while i < UNALLOCATED_BUFFER_ITEMS
	{
		cur: = cast(**mem_chunk) (ptr_offset(alloc.probable_unallocated, i, *mem_chunk));
		if* cur == nil
		{
			*cur = a;
			break;
		}
		i++;
	}
}
ASSERT::fn macro(exp : _expr)
{
	if !(exp)
		__dbg_break;
}
ptr_offset::fn macro(ptr : _expr, offset : _expr, type : _expr)
{
	(cast(*type)(cast(u64)ptr + (offset) * sizeof(type)))
}
is_flag_off::fn macro(val : _expr, flag : _expr)
{
	((val & flag) == 0)
}
is_flag_on::fn macro(val : _expr, flag : _expr)
{
	((val & flag) != 0)
}
_own_memset::fn(dst : *u8, val : u8, sz : u64) !void
{
	ASSERT(cast(u64)dst > 128);
	i:u64 = 0;
	while i < sz
	{
		*ptr_offset(dst, i, u8) = val;
		i++;
	}
}
memcpy::fn(dst : *u8, src : *u8, sz : u64) !void
{
	ASSERT(cast(u64)dst > 128 && cast(u64)src > 128) ;
	i:u64 = 0;
	while i < sz
	{
		*ptr_offset(dst, i, u8) = *ptr_offset(src, i, u8);
		i++;
	}
}
memset::fn(dst : *u8, val : u8, sz : u64) !void
{
	ASSERT(cast(u64)dst > 128);
	i:u64 = 0;
	while i < sz
	{
		*ptr_offset(dst, i, u8) = val;
		i++;
	}
}

GetUnallocatedChunk::fn(alloc : *mem_alloc) !*mem_chunk
{
	ret:*mem_chunk = heap_get_unallocated_cache(alloc);

	if ret
	{

		heap_insert_unallocated(alloc, ptr_offset(ret, 1, mem_chunk));
		heap_insert_unallocated(alloc, ptr_offset(ret, -1, mem_chunk));
		return ret;
	}
	if !ret
	{
		i:u64 = 0;
		while i < CHUNKS_CAP
		{
			cur0:*mem_chunk = ptr_offset(alloc.all, i, mem_chunk);

			if is_flag_off(cur0.flags, CHUNK_ALLOCATED)
			{
				ret = cur0;

				heap_insert_unallocated(alloc, cast(*mem_chunk)(cast(u64)ptr_offset(alloc.all, (i + 1) % CHUNKS_CAP, mem_chunk)));
				break;
			}
			i++;
		}
	}
	ret.flags = ret.flags | CHUNK_ALLOCATED;

	ret.next = nil;
	ret.prev = nil;
    return ret;
}
HHashStore :: fn (self: *heap_hash, key : *u8, value : *u8)
{
	using self;
	idx : u64  = (cast(u64)key) % HASH_TABLE_SIZE;
	cur:= ptr_offset(data, idx, inner);

	if cur.key == nil
	{
		cur.key = key;
		cur.value = value;
		return;
	}
	put:bool = false;
	i:u64=0;
	while i < HASH_TABLE_SIZE
	{
		mod:u64 = (i + idx + 1) % HASH_TABLE_SIZE;
		cur= ptr_offset(data, mod, inner);

		if cur.key == nil
		{
			cur.key = key;
			cur.value = value;
			put = true;
			break;
		}
		i++;
	}
}
HHashClear :: fn(self : *heap_hash) ! void
{
	using self;
	memset(cast(*u8)data, 0, sizeof(inner) * HASH_TABLE_SIZE);
}
HHashGet :: fn (self: *heap_hash, key : *u8) ! *u8 
{
	using self;
	idx : u64  = (cast(u64)key) % HASH_TABLE_SIZE;
	cur:= ptr_offset(data, idx, inner);

	if cur.key == key
		return cur.value;

	put:bool = false;
	i:u64=0;
	while i < HASH_TABLE_SIZE
	{
		mod:u64 = (i + idx + 1) % HASH_TABLE_SIZE;
		cur= ptr_offset(data, mod, inner);

		if cur.key == key
			return cur.value;
		i++;
	}
	ASSERT(0 == 1);
	return nil;
}
HHashRemove :: fn (self: *heap_hash, key : *u8)
{
	using self;
	idx : u64  = (cast(u64)key) % HASH_TABLE_SIZE;
	cur:= ptr_offset(data, idx, inner);

	if cur.key == key
	{
		cur.key = nil;
		cur.value = nil;
		return;
	}
	removed:bool = false;
	i:u64=0;
	while i < HASH_TABLE_SIZE
	{
		mod:u64 = (i + idx + 1) % HASH_TABLE_SIZE;
		cur= ptr_offset(data, mod, inner);

		if (cur.key == key)
		{
			cur.key = nil;
			removed = true;
			break;
		}
		i++;
	}
	ASSERT(removed);
}
heap_get_unallocated_cache :: fn(alloc : *mem_alloc) ! *mem_chunk
{
	i:u64= 0;
	while i < UNALLOCATED_BUFFER_ITEMS
	{
		cur := cast(**mem_chunk) (ptr_offset(alloc.probable_unallocated, i, *mem_chunk));
		if* cur != nil
		{
			ret :*mem_chunk = *cur;
			cur.flags = cur.flags | CHUNK_ALLOCATED;
			*cur = nil;
			ret.next = nil;
			ret.prev = nil;
			return ret;
		}
		i++;
	}
	return nil;
}
// joins two contiguos chunks
maybe_join_to_right::fn(prev : *mem_chunk, cur : *mem_chunk, next : *mem_chunk)
{
	contiguous_with_cur_next := (cast(u64)cur.addr + cur.size * BYTES_PER_CHUNK) == cast(u64)next.addr;
	if contiguous_with_cur_next
	{
		cur.size += next.size;
		next_aux :*mem_chunk= next;

		next.flags = next.flags & ~CHUNK_ALLOCATED;

		cur.next = next.next;
		cur.prev = prev;
		if next.next
			next.next.prev = cur;
		if prev
			prev.next = cur;
	}
}
heap_clear::fn(alloc : *mem_alloc)
{
	HHashClear(&alloc.in_use);
	total_size :u32= CHUNKS_CAP * BYTES_PER_CHUNK;
	memset(cast(*u8)alloc.all, 0, cast(u64)total_size);

    free:*mem_chunk = GetUnallocatedChunk(alloc);

	free.size = total_size / BYTES_PER_CHUNK;
    free.addr = alloc.buffer;

	alloc.head_free = free;
}
heap_free::fn(alloc : *mem_alloc, ptr : *u8)
{
    chunk_to_free:= cast(*mem_chunk)HHashGet(&alloc.in_use, ptr);
	ASSERT(chunk_to_free != nil);
	HHashRemove(&alloc.in_use, ptr);
    cur :*mem_chunk= alloc.head_free;
	
	
	// in case the chunk to free is in a higher address want
	// we want to find in front of which chunk are we in
	// and increment its size
	if (chunk_to_free.addr > cur.addr)
	{
		while (cur.next != nil && cur.next.addr < chunk_to_free.addr)
		{
			cur = cur.next;
		}

		ASSERT(cur != nil && cur.next.addr >= chunk_to_free.addr);
		
		//TODO: create a var put these two vals in each bit, and create a swich statetement so that we 
		// wont need if and elses
		contiguous_with_cur:bool = (cast(u64)cur.addr + cur.size * BYTES_PER_CHUNK) == cast(u64)chunk_to_free.addr;
		contiguous_with_cur_next:bool = (cast(u64)chunk_to_free.addr + chunk_to_free.size * BYTES_PER_CHUNK) == cast(u64)cur.next.addr;


		// to join with cur, the chunk_free and cur needs to be contiguous
		if (contiguous_with_cur && !contiguous_with_cur_next)
		{
			cur.size += chunk_to_free.size;
			chunk_to_free.flags = chunk_to_free.flags & ~CHUNK_ALLOCATED;
			heap_insert_unallocated(alloc, chunk_to_free);
		}
		// we migth also want also to join with the cur.next
		else if (contiguous_with_cur_next && !contiguous_with_cur)
		{
			maybe_join_to_right(cur, chunk_to_free, cur.next);


		}
		// if we're contiguous with both
		else if (contiguous_with_cur_next && contiguous_with_cur)
		{
			cur.size += chunk_to_free.size + cur.next.size;

			cur.next.flags = cur.next.flags & ~CHUNK_ALLOCATED;
			chunk_to_free.flags = chunk_to_free.flags & ~CHUNK_ALLOCATED;
			heap_insert_unallocated(alloc, cur.next);
			heap_insert_unallocated(alloc, chunk_to_free);



			if (cur.next.next)
			{
				next_next:*mem_chunk = cur.next.next;
				cur.next = cur.next.next;
				next_next.prev = cur;
			}
			else
				cur.next = nil;

		}
		// if we're not contiguos with anything
		else if(!contiguous_with_cur_next && !contiguous_with_cur)
		{
			next_aux:*mem_chunk = cur.next;

			cur.next = chunk_to_free;
			chunk_to_free.prev = cur;
			chunk_to_free.next = next_aux;
			next_aux.prev = chunk_to_free;
			//chunk_to_free.flags &= ~CHUNK_IN_USE;
		}
	}
	else
	{
		chunk_to_free.prev = nil;
		// making the chunk to be the head
		chunk_to_free.next = cur;
		cur.prev = chunk_to_free;
		//__dbg_break;
		alloc.head_free = chunk_to_free;

		maybe_join_to_right(nil, chunk_to_free, cur);
		//__dbg_break;
	}
	//memset(chunk_to_free.addr, 'x', chunk_to_free.size * BYTES_PER_CHUNK);

}
heap_alloc::fn(alloc : *mem_alloc, size : u32) !*void
{
	//return (char *)malloc(size);
    cur:*mem_chunk= alloc.head_free;

    while cur != nil && (cur.size * BYTES_PER_CHUNK) < size
    {
        cur = cur.next;
    }
    ASSERT(cur != nil && (cur.size * BYTES_PER_CHUNK) >= size);

	got_all_bytes_from_chunk:bool= ((cur.size * BYTES_PER_CHUNK) - size) == 0;
	
	min_alloc_size:= size;

	if(min_alloc_size  == 0)
		min_alloc_size = 1 * BYTES_PER_CHUNK;

	if (got_all_bytes_from_chunk)
	{
		if (cur.prev)
			cur.prev.next = cur.next;
		if (cur.next)
			cur.next.prev = cur.prev;

		if (alloc.head_free == cur)
			alloc.head_free = cur.next;
		//cur.flags &= ~CHUNK_ALLOCATED;
		alloc.head_free = cur.next;
	}
	else
	{
		// fragmenting the original chunk
		other_chunk:*mem_chunk= GetUnallocatedChunk(alloc);
		ASSERT(other_chunk != cur);
		

		align_to_bytes_per_chunk:= (min_alloc_size % BYTES_PER_CHUNK);
		// if we're not aligned yet
		if (align_to_bytes_per_chunk != 0)
			align_to_bytes_per_chunk = BYTES_PER_CHUNK - align_to_bytes_per_chunk;

		min_alloc_size += align_to_bytes_per_chunk;

		other_chunk.addr = ptr_offset(cur.addr, min_alloc_size, u8);
		other_chunk.size = cur.size - min_alloc_size / BYTES_PER_CHUNK;
		if (cur.prev)
		{
			prev:= cur.prev;
			prev.next = other_chunk;
			other_chunk.prev = prev;

		}
		if (cur.next)
		{
			next := cur.next;
			next.prev = other_chunk;
			other_chunk.next = next;
		}

		if(cur == alloc.head_free)
			alloc.head_free = other_chunk;
	}

    //cur.flags |= CHUNK_IN_USE;
	HHashStore(&alloc.in_use, cur.addr, cast(*u8)cur);
	cur.size = min_alloc_size / BYTES_PER_CHUNK;

	ASSERT(cur.next != cur);
	//if (out)
		//*out = cur;
    return cur.addr;
}
InitMemAlloc::fn(alloc : *mem_alloc)
{
    all_chunks_sz:u64= CHUNKS_CAP * sizeof(mem_chunk);
    alloc.all = cast(*mem_chunk)GetMem(cast(u64)all_chunks_sz);
    memset(cast(*u8)alloc.all, 0, all_chunks_sz);
	//__dbg_break;

	unallocated_sz:u64 = UNALLOCATED_BUFFER_ITEMS * 8;
	alloc.probable_unallocated = cast(**mem_chunk)GetMem(cast(u64)unallocated_sz);
    memset(cast(*u8)alloc.probable_unallocated, 0, unallocated_sz);

    total_size:u64= CHUNKS_CAP * BYTES_PER_CHUNK;
	alloc.buffer = cast(*u8)GetMem(total_size);
	
	in_use_hash_sz:u64 = HASH_TABLE_SIZE * sizeof(heap_hash.inner);
	alloc.in_use.data = cast(*heap_hash.inner)GetMem(cast(u64)in_use_hash_sz);
	memset(cast(*u8)alloc.in_use.data, 0, in_use_hash_sz);

    free:*mem_chunk= GetUnallocatedChunk(alloc);
	//alloc.in_use.reserve(HASH_TABLE_SIZE);

	free.size = cast(u32)(total_size / BYTES_PER_CHUNK);
    free.addr = alloc.buffer;


	alloc.head_free = free;
	a: = 0;
}



test_call2::fn(alloc : *mem_alloc)
{
	*ptr_offset(alloc.buffer, 3, u8) = 4;
}
test_call1::fn(alloc : *mem_alloc)
{
	*ptr_offset(alloc.buffer, 2, u8) = 3;
	test_call2(alloc);
}
test_ref::fn(alloc : *mem_alloc)
{
	*ptr_offset(alloc.buffer, 2, u8) = 2;
}
test_init ::fn(alloc : *mem_alloc)
{
	alloc.buffer = cast(*u8)GetMem(8);
}
test_change_heap_hash::fn(h : *heap_hash)
{
	h.data = cast(*heap_hash.inner)100;
}

test_sum_float::fn(f1 : f32, f2 : f32) !f32
{
	return f1 + f2;
}

test_ptr::fn(alloc : *mem_alloc)
{

	test_change_heap_hash(&alloc.in_use);
	if (cast(u64)alloc.in_use.data) != 100
		__dbg_break;

	in_use_hash_sz:u64 = HASH_TABLE_SIZE * sizeof(heap_hash.inner);
	alloc.in_use.data = cast(*heap_hash.inner)GetMem(cast(u64)in_use_hash_sz);
	memset(cast(*u8)alloc.in_use.data, 0, in_use_hash_sz);


	if alloc.in_use.data == nil
		__dbg_break;


	chunk1:mem_chunk;

	base_addr: = cast(**mem_chunk)GetMem(8 * 8);
	alloc.probable_unallocated = base_addr;
	*ptr_offset(alloc.probable_unallocated, 0, *mem_chunk) = nil;

	if* ptr_offset(alloc.probable_unallocated, 0, *mem_chunk) != nil
		__dbg_break;

	alloc.probable_unallocated = base_addr;
	if alloc.probable_unallocated != base_addr
		__dbg_break;

	*ptr_offset(alloc.probable_unallocated, 0, *mem_chunk) = &chunk1;


	if alloc.probable_unallocated != base_addr
		__dbg_break;

	if alloc.probable_unallocated == cast(**mem_chunk) &chunk1
		__dbg_break;

	if *ptr_offset(alloc.probable_unallocated, 0, *mem_chunk) == nil
		__dbg_break;

	ptr_offset(alloc.probable_unallocated, 0, *mem_chunk).size = 2;

	if chunk1.size != 2
		__dbg_break;

	cur: = ptr_offset(alloc.probable_unallocated, 0, *mem_chunk);

	if *cur == nil
		__dbg_break;

	if cur.size != 2
		__dbg_break;

	*cur = nil;

	if *cur
		__dbg_break;


	HHashStore(&alloc.in_use, cast(*u8)1, cast(*u8)5);
	HHashStore(&alloc.in_use, cast(*u8)513, cast(*u8)6);

    val := cast(u64)HHashGet(&alloc.in_use, cast(*u8)1);
    val2 := cast(u64)HHashGet(&alloc.in_use, cast(*u8)513);
	if val != 5 || val2 != 6
		__dbg_break;

	ptr1:= cast(*mem_chunk)GetMem(sizeof(mem_chunk));
	ptr2:= cast(*mem_chunk)GetMem(sizeof(mem_chunk));

	ptr1.addr = cast(*u8)10;
	ptr2.addr = cast(*u8)20;

	if ptr1.addr > ptr2.addr
		__dbg_break;

	ptr1.size = 5;
	ptr2.size = 5;

	ptr1.size += ptr2.size;

	if ptr1.size  != 10
		__dbg_break;

	contiguous_with_ptr2 := (cast(u64)ptr1.addr + 10) == cast(u64)ptr2.addr;

	if contiguous_with_ptr2 != cast(bool)1
		__dbg_break;
	
	contiguous_with_ptr2 = false;

	ptr1.size = 5;
	contiguous_with_ptr2 = (cast(u64)ptr1.addr + ptr1.size * 2) == cast(u64)ptr2.addr;
	if contiguous_with_ptr2 != cast(bool)1
		__dbg_break;
	
	*cur = &chunk1;
	cur.flags = CHUNK_ALLOCATED;

	if is_flag_off(cur.flags, CHUNK_ALLOCATED)
		__dbg_break;
	
}
nil_ptr_ret::fn() !*void
{
	return nil;
}
simple_strct: struct
{
	a : u32,
	b: u32,
}
func_ptr_test::fn() !u8
{
	return 2;
}
tests :: fn()
{
	i:= 0;
	if i != 0
		__dbg_break;

	while i < 5
	{
		i++;
	}

	if i != 5
		__dbg_break;

	while true
	{
		if i > 10
			break;
		i++;
	}
	if i != 11
		__dbg_break;

	alloc :mem_alloc;
	alloc.buffer = nil;

	if alloc.buffer 
		__dbg_break;

	alloc.buffer = cast(*u8)GetMem(8);

	*ptr_offset(alloc.buffer, 1, u8) = 1;

	if *ptr_offset(alloc.buffer, 1, u8) != 1
		__dbg_break;

	test_ref(&alloc);

	if *ptr_offset(alloc.buffer, 2, u8) != 2
		__dbg_break;

	*ptr_offset(alloc.buffer, 1, u8) = 0;
	*ptr_offset(alloc.buffer, 2, u8) = 0;
	test_call1(&alloc);


	alloc.buffer = nil;

	if alloc.buffer != nil
		__dbg_break;

	test_init(&alloc);

	if alloc.buffer == nil
		__dbg_break;

	*ptr_offset(alloc.buffer, 1, u8) = 7;

	if *ptr_offset(alloc.buffer, 1, u8) != 7
		__dbg_break;

	memset(alloc.buffer, 0, 8);

	if *ptr_offset(alloc.buffer, 1, u8) != 0
		__dbg_break;

	test_ptr(&alloc);

	flags :u64= 0;
	flags = flags | 1;

	if flags != 1
		__dbg_break;

	flags = flags & ~1;
	
	if flags != 0
		__dbg_break;

	flags = 2;
	flags = flags | 1;

	if flags != 3
		__dbg_break;

	flags = flags & ~2;
	if flags != 1
		__dbg_break;

	i = 6;
	i = i % 7;

	if i != 6
		__dbg_break;

	if (i + 1 % 7) != 0
		__dbg_break;

	bool_v: = false;

	if bool_v
		__dbg_break;

	i = 5;
	bool_v = i == 5;


	if !bool_v
		__dbg_break;

	i = 4;
	i2 := 5;
	i3 := 5;
	i += i2 + i3;

	if i != 14
		__dbg_break;

	ptr: *mem_chunk = nil;

	if ptr
		__dbg_break;

	ptr = cast(*mem_chunk)GetMem(32);

	if ptr == nil
		__dbg_break;

	ptr = cast(*mem_chunk)nil_ptr_ret();

	if ptr != nil
		__dbg_break;

	if sizeof(simple_strct) != 8
		__dbg_break;

	if cast(u64)ptr_offset(0, 0, simple_strct) != 0
		__dbg_break;
	if cast(u64)ptr_offset(0, 1, simple_strct) != 8
		__dbg_break;
	if cast(u64)ptr_offset(0, 2, simple_strct) != 16
		__dbg_break;
	if cast(u64)ptr_offset(0, 0, *simple_strct) != 0
		__dbg_break;
	if cast(u64)ptr_offset(0, 1, *simple_strct) != 8
		__dbg_break;
	if cast(u64)ptr_offset(0, 2, *simple_strct) != 16
		__dbg_break;

	main_type: fn() !u8;
	main_type = fn() !u8{ return 1; };

	if main_type() != 1
		__dbg_break;

	main_type = func_ptr_test;
	if main_type() != 2
		__dbg_break;

	f := 1.0;

	if f > 2.0
		__dbg_break;


	greater_than_0 : = f > 0.0;
	less_than_3 : = f < 3.0;
	greater_than_2 : = f > 2.0;

	val:bool = false;
	if greater_than_0 && !greater_than_2
		val = true;

	if val == false
		__dbg_break;

	val = false;

	if !greater_than_0 || !greater_than_2
		val = true;

	if val == false
		__dbg_break;

	val = false;

	if greater_than_0 && !greater_than_2 && less_than_3
		val = true;

	if val == false
		__dbg_break;

	val = false;
	if greater_than_0 == false || greater_than_2 || less_than_3
		val = true;

	if val == false
		__dbg_break;

	val = false;

	if !(greater_than_2 && less_than_3)
		val = true;

	if val == false
		__dbg_break;
	
	fsum: = test_sum_float(1.0, 2.0);
	if fsum < 2.0  || fsum > 4.0
		__dbg_break;

	chunk: = mem_chunk{ next: nil, prev : cast(*mem_chunk)7 };
	if cast(u64)chunk.prev != 7
		__dbg_break;


}
